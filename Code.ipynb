{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9995ebfb-f13c-48a6-aed4-a532f2b1b71c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 157 layers, 1763224 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Two-Stage Pipeline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 11:51:30.504 python[20354:1010707] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-03-29 11:51:30.504 python[20354:1010707] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two-Stage Pipeline:\n",
      "  First-stage (Nano) inference time: 0.05 seconds\n",
      "  Second-stage (Large) inference time: 0.09 seconds\n",
      "\n",
      "--- Running Single-Stage Pipeline ---\n",
      "Single-Stage Pipeline:\n",
      "  YOLOv5 Large inference time: 0.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "yolo_nano = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/nano_weights/best.pt')  # First-stage model\n",
    "yolo_large = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/large_weights/best.pt')  # Second-stage model\n",
    "\n",
    "# Two-stage detection pipeline\n",
    "def two_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # First-stage detection (YOLOv5 Nano)\n",
    "    start_time_nano = time.time()\n",
    "    results_nano = yolo_nano(image_rgb)\n",
    "    end_time_nano = time.time()\n",
    "    detections_nano = results_nano.xyxy[0]  # [x1, y1, x2, y2, confidence, class]\n",
    "\n",
    "    verified_detections = []\n",
    "\n",
    "    # Second-stage verification (YOLOv5 Large)\n",
    "    start_time_large = time.time()\n",
    "    for det in detections_nano:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "        \n",
    "        # Confidence threshold for Nano\n",
    "        if conf > 0.25:  # Adjust threshold as needed\n",
    "            # Crop region of interest (ROI) for second-stage model\n",
    "            crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Skip if crop is empty\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert crop to RGB format\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Run YOLOv5 Large on the cropped region without resizing\n",
    "            results_large = yolo_large(crop_rgb, size=crop_rgb.shape[0])  # Use height of crop as size\n",
    "            detections_large = results_large.xyxy[0]\n",
    "\n",
    "            # Confirm detections\n",
    "            for det_large in detections_large:\n",
    "                x1_l, y1_l, x2_l, y2_l, conf_l, cls_l = det_large.tolist()\n",
    "                \n",
    "                if conf_l > 0.5:  # Adjust threshold for Large\n",
    "                    # Map detection back to original coordinates\n",
    "                    verified_detections.append([x1, y1, x2, y2, conf_l, cls_l])\n",
    "    end_time_large = time.time()\n",
    "\n",
    "    # Visualize or save results\n",
    "    for x1, y1, x2, y2, conf, cls in verified_detections:\n",
    "        label = yolo_large.names[int(cls)]\n",
    "        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "    # Show the final image\n",
    "    cv2.imshow('Detections (Two-Stage)', image)\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print inference times\n",
    "    print(f\"Two-Stage Pipeline:\")\n",
    "    print(f\"  First-stage (Nano) inference time: {end_time_nano - start_time_nano:.2f} seconds\")\n",
    "    print(f\"  Second-stage (Large) inference time: {end_time_large - start_time_large:.2f} seconds\")\n",
    "\n",
    "# Single-stage detection using YOLOv5 Large\n",
    "def single_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Large model detection without resizing to 640\n",
    "    start_time = time.time()\n",
    "    results_large = yolo_large(image_rgb, size=image_rgb.shape[0])  # Avoid resizing to 640\n",
    "    end_time = time.time()\n",
    "\n",
    "    detections_large = results_large.xyxy[0]  # [x1, y1, x2, y2, confidence, class]\n",
    "\n",
    "    # Visualize or save results\n",
    "    for det in detections_large:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "        if conf > 0.5:  # Confidence threshold for Large\n",
    "            label = yolo_large.names[int(cls)]\n",
    "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the final image\n",
    "    cv2.imshow('Detections (Single-Stage)', image)\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print inference time\n",
    "    print(f\"Single-Stage Pipeline:\")\n",
    "    print(f\"  YOLOv5 Large inference time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Test pipeline\n",
    "image_path = '/Users/ashish/Desktop/final_projects/suspicious activityy/dataset/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/model_1_data/images/test/pistol_9019.jpg'\n",
    "print(\"\\n--- Running Two-Stage Pipeline ---\")\n",
    "two_stage_detection_pipeline(image_path)\n",
    "\n",
    "print(\"\\n--- Running Single-Stage Pipeline ---\")\n",
    "single_stage_detection_pipeline(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f24b346-002a-4f73-b219-14f504ac5059",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 157 layers, 1763224 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measuring inference times...\n",
      "Processing img1261.jpg...\n",
      "Processing billete_2221.jpg...\n",
      "Processing knife_1043.jpg...\n",
      "Processing monedero_1133.jpg...\n",
      "Processing HBmframe00163.jpg...\n",
      "Processing knife_16.jpg...\n",
      "Processing billete_2194.jpg...\n",
      "Processing smartphone_1041.jpg...\n",
      "Processing img6452.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques185.jpg...\n",
      "Processing smartphone_1069.jpg...\n",
      "Processing monedero_1523.jpg...\n",
      "Processing billete_2355.jpg...\n",
      "Processing ABmframe00262.jpg...\n",
      "Processing pistol_5076.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques153.jpg...\n",
      "Processing smartphone_z3v2_28.jpg...\n",
      "Processing knife_1136.jpg...\n",
      "Processing monedero_1522.jpg...\n",
      "Processing knife_1122.jpg...\n",
      "Processing smartphone_1068.jpg...\n",
      "Processing knife_576.jpg...\n",
      "Processing img1102.jpg...\n",
      "Processing ABsframe00208.jpg...\n",
      "Processing knife_1308.jpg...\n",
      "Processing knife_600.jpg...\n",
      "Processing img859.jpg...\n",
      "Processing knife_358.jpg...\n",
      "Processing knife_416.jpg...\n",
      "Processing billete_2208.jpg...\n",
      "Processing knife_402.jpg...\n",
      "Processing knife_1054.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques757.jpg...\n",
      "Processing smartphone_z3v7_26.jpg...\n",
      "Processing img2233.jpg...\n",
      "Processing knife_1242.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques233.jpg...\n",
      "Processing knife_825.jpg...\n",
      "Processing KravMagaTraining20584.jpg...\n",
      "Processing tarjeta_2512.jpg...\n",
      "Processing DefenseKnifeAttack0476.jpg...\n",
      "Processing smartphone_1042.jpg...\n",
      "Processing knife_574.jpg...\n",
      "Processing img7773.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques151.jpg...\n",
      "Processing DefenseKnifeAttack1224.jpg...\n",
      "Processing smartphone_z3v2_02.jpg...\n",
      "Processing knife_549.jpg...\n",
      "Processing sharpenKnife298.jpg...\n",
      "Processing smartphone_1043.jpg...\n",
      "Processing knife_1323.jpg...\n",
      "Processing knife_830.jpg...\n",
      "Processing KravMagaTraining565.jpg...\n",
      "Processing DSC_00321.jpg...\n",
      "Processing img6732.jpg...\n",
      "Processing knife_617.jpg...\n",
      "Processing KravMagaTraining4045.jpg...\n",
      "Processing img641.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques024.jpg...\n",
      "Processing LBsframe00361.jpg...\n",
      "Processing knife_1041.jpg...\n",
      "Processing img4333.jpg...\n",
      "Processing img1273.jpg...\n",
      "Processing knife_1051.jpg...\n",
      "Processing billete_2233.jpg...\n",
      "Processing img651.jpg...\n",
      "Processing KravMagaTraining4280.jpg...\n",
      "Processing knife_161.jpg...\n",
      "Processing knife_820.jpg...\n",
      "Processing img1139.jpg...\n",
      "Processing DefenseKnifeAttack0315.jpg...\n",
      "Processing img3262.jpg...\n",
      "Processing knife_565.jpg...\n",
      "Processing smartphone_1053.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques155.jpg...\n",
      "Processing knife_564.jpg...\n",
      "Processing smartphone_1046.jpg...\n",
      "Processing RealityKnifeAttacks0651.jpg...\n",
      "Processing billete_2352.jpg...\n",
      "Processing billete_2193.jpg...\n",
      "Processing knife_809.jpg...\n",
      "Processing img452.jpg...\n",
      "Processing KravMagaTraining21067.jpg...\n",
      "Processing knife_160.jpg...\n",
      "Processing img3115.jpg...\n",
      "Processing ABsframe00193.jpg...\n",
      "Processing knife_1087.jpg...\n",
      "Processing knife_404.jpg...\n",
      "Processing knife_410.jpg...\n",
      "Processing KravMagaTraining20812.jpg...\n",
      "Processing img6251.jpg...\n",
      "Processing knife_412.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques023.jpg...\n",
      "Processing knife_638.jpg...\n",
      "Processing ABbframe00145.jpg...\n",
      "Processing knife_604.jpg...\n",
      "Processing img1728.jpg...\n",
      "Processing img3659.jpg...\n",
      "Processing knife_1287.jpg...\n",
      "Processing HBmframe00172.jpg...\n",
      "Processing img3711.jpg...\n",
      "Processing sharpenKnife089.jpg...\n",
      "Processing knife_1318.jpg...\n",
      "Processing billete_2191.jpg...\n",
      "Processing img2396.jpg...\n",
      "Processing smartphone_1050.jpg...\n",
      "Processing smartphone_1044.jpg...\n",
      "Processing KravMagaTraining614.jpg...\n",
      "Processing pistola_z3v7_05.jpg...\n",
      "Processing smartphone_1045.jpg...\n",
      "Processing smartphone_1051.jpg...\n",
      "Processing img1463.jpg...\n",
      "Processing knife_1127.jpg...\n",
      "Processing billete_2190.jpg...\n",
      "Processing KnifeDefenseKrav078.jpg...\n",
      "Processing knife_995.jpg...\n",
      "Processing knife_1286.jpg...\n",
      "Processing smartphone_9009.jpg...\n",
      "Processing knife_1084.jpg...\n",
      "Processing img653.jpg...\n",
      "Processing billete_2219.jpg...\n",
      "Processing knife_349.jpg...\n",
      "Processing pistol_9001.jpg...\n",
      "Processing img2904.jpg...\n",
      "Processing pistol_9029.jpg...\n",
      "Processing knife_110.jpg...\n",
      "Processing knife_138.jpg...\n",
      "Processing knife_1236.jpg...\n",
      "Processing tarjeta_2374.jpg...\n",
      "Processing knife_61.jpg...\n",
      "Processing DSC_00591.jpg...\n",
      "Processing HBsframe00235.jpg...\n",
      "Processing knife_919.jpg...\n",
      "Processing MBmframe00130.jpg...\n",
      "Processing knife_925.jpg...\n",
      "Processing img4232.jpg...\n",
      "Processing knife_1168.jpg...\n",
      "Processing smartphone_1022.jpg...\n",
      "Processing DefenseKnifeAttack0172.jpg...\n",
      "Processing pistol_5000.jpg...\n",
      "Processing knife_298.jpg...\n",
      "Processing knife_529.jpg...\n",
      "Processing sharpenKnife039.jpg...\n",
      "Processing knife_74.jpg...\n",
      "Processing knife_139.jpg...\n",
      "Processing tarjeta_2407.jpg...\n",
      "Processing pistol_9028.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques078.jpg...\n",
      "Processing KravMagaTraining049.jpg...\n",
      "Processing img2050.jpg...\n",
      "Processing ABbframe00322.jpg...\n",
      "Processing knife_113.jpg...\n",
      "Processing tarjeta_2377.jpg...\n",
      "Processing tarjeta_2405.jpg...\n",
      "Processing tarjeta_2363.jpg...\n",
      "Processing knife_1341.jpg...\n",
      "Processing KravMagaTraining467.jpg...\n",
      "Processing smartphone_1021.jpg...\n",
      "Processing smartphone_1035.jpg...\n",
      "Processing knife_1157.jpg...\n",
      "Processing smartphone_1009.jpg...\n",
      "Processing monedero_1581.jpg...\n",
      "Processing pistol_5003.jpg...\n",
      "Processing smartphone_1008.jpg...\n",
      "Processing img2127.jpg...\n",
      "Processing pistola_z1v14_07.jpg...\n",
      "Processing pistola_z3v3_10.jpg...\n",
      "Processing knife_728.jpg...\n",
      "Processing knife_853.jpg...\n",
      "Processing HBbframe00181.jpg...\n",
      "Processing tarjeta_2362.jpg...\n",
      "Processing pistol_9003.jpg...\n",
      "Processing pistol_9017.jpg...\n",
      "Processing LBsframe00316.jpg...\n",
      "Processing img7665.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques090.jpg...\n",
      "Processing knife_466.jpg...\n",
      "Processing knife_664.jpg...\n",
      "Processing knife_67.jpg...\n",
      "Processing knife_857.jpg...\n",
      "Processing tarjeta_2399.jpg...\n",
      "Processing MBsframe00439.jpg...\n",
      "Processing img1364.jpg...\n",
      "Processing smartphone_1018.jpg...\n",
      "Processing smartphone_1024.jpg...\n",
      "Processing img2876.jpg...\n",
      "Processing monedero_1585.jpg...\n",
      "Processing pistol_5013.jpg...\n",
      "Processing LBsframe00298.jpg...\n",
      "Processing pistol_5012.jpg...\n",
      "Processing pistol_5006.jpg...\n",
      "Processing knife_1190.jpg...\n",
      "Processing smartphone_1025.jpg...\n",
      "Processing knife_507.jpg...\n",
      "Processing knife_1345.jpg...\n",
      "Processing KravMagaKnifeDefenseTechniques254.jpg...\n",
      "Processing sharpenKnife163.jpg...\n",
      "Processing DefenseAndSurvive14.jpg...\n",
      "Processing knife_103.jpg...\n",
      "Processing img1749.jpg...\n",
      "Processing img4157.jpg...\n",
      "Processing tarjeta_2367.jpg...\n",
      "Processing pistol_9012.jpg...\n",
      "Processing DefenseKnifeAttack0798.jpg...\n",
      "Processing knife_1019.jpg...\n",
      "Processing pistol_9010.jpg...\n",
      "Processing knife_667.jpg...\n",
      "Processing KravMagaTraining4035.jpg...\n",
      "Processing knife_908.jpg...\n",
      "Processing knife_539.jpg...\n",
      "Processing smartphone_1033.jpg...\n",
      "Processing MBbframe00388.jpg...\n",
      "Processing KravMagaTraining105.jpg...\n",
      "Processing img2875.jpg...\n",
      "Processing knife_510.jpg...\n",
      "Processing smartphone_1032.jpg...\n",
      "Processing knife_538.jpg...\n",
      "Processing smartphone_z3v8_07.jpg...\n",
      "Processing sharpenKnife028.jpg...\n",
      "Processing KravMagaTraining21167.jpg...\n",
      "Processing img3820.jpg...\n",
      "Processing knife_869.jpg...\n",
      "Processing img2294.jpg...\n",
      "Processing KravMagaTraining4034.jpg...\n",
      "Processing KravMagaTraining4593.jpg...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Ensure the YOLO models are loaded\n",
    "import torch\n",
    "yolo_nano = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/nano_weights/best.pt')  # First-stage model\n",
    "yolo_large = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/large_weights/best.pt')  # Second-stage model\n",
    "\n",
    "# Directory containing images\n",
    "image_directory = '/Users/ashish/Desktop/final_projects/suspicious activityy/dataset/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/model_1_data/images/test'\n",
    "\n",
    "# Two-stage detection pipeline\n",
    "def two_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return 0  # Handle invalid image\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # First-stage detection (YOLOv5 Nano)\n",
    "    start_time_nano = time.time()\n",
    "    results_nano = yolo_nano(image_rgb)\n",
    "    end_time_nano = time.time()\n",
    "    detections_nano = results_nano.xyxy[0]\n",
    "\n",
    "    verified_detections = []\n",
    "\n",
    "    # Second-stage verification (YOLOv5 Large)\n",
    "    start_time_large = time.time()\n",
    "    for det in detections_nano:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "\n",
    "        # Confidence threshold for Nano\n",
    "        if conf > 0.25:\n",
    "            # Crop region of interest (ROI)\n",
    "            crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Convert crop to RGB format\n",
    "            crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Run YOLOv5 Large on the cropped region\n",
    "            results_large = yolo_large(crop_rgb, size=crop_rgb.shape[0])\n",
    "            detections_large = results_large.xyxy[0]\n",
    "\n",
    "            for det_large in detections_large:\n",
    "                x1_l, y1_l, x2_l, y2_l, conf_l, cls_l = det_large.tolist()\n",
    "                if conf_l > 0.5:\n",
    "                    verified_detections.append([x1, y1, x2, y2, conf_l, cls_l])\n",
    "    end_time_large = time.time()\n",
    "\n",
    "    # Return total inference time\n",
    "    return (end_time_nano - start_time_nano) + (end_time_large - start_time_large)\n",
    "\n",
    "# Single-stage detection pipeline\n",
    "def single_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return 0  # Handle invalid image\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Large model detection\n",
    "    start_time = time.time()\n",
    "    results_large = yolo_large(image_rgb, size=image_rgb.shape[0])\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Return inference time\n",
    "    return end_time - start_time\n",
    "\n",
    "# Measure average inference times\n",
    "def measure_inference_times():\n",
    "    two_stage_times = []\n",
    "    single_stage_times = []\n",
    "\n",
    "    # Process all images in the directory\n",
    "    for filename in os.listdir(image_directory):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_path = os.path.join(image_directory, filename)\n",
    "            print(f\"Processing {filename}...\")\n",
    "\n",
    "            # Measure two-stage pipeline time\n",
    "            time_two_stage = two_stage_detection_pipeline(image_path)\n",
    "            two_stage_times.append(time_two_stage)\n",
    "\n",
    "            # Measure single-stage pipeline time\n",
    "            time_single_stage = single_stage_detection_pipeline(image_path)\n",
    "            single_stage_times.append(time_single_stage)\n",
    "\n",
    "    # Calculate average times\n",
    "    avg_two_stage = sum(two_stage_times) / len(two_stage_times) if two_stage_times else 0\n",
    "    avg_single_stage = sum(single_stage_times) / len(single_stage_times) if single_stage_times else 0\n",
    "\n",
    "    return avg_two_stage, avg_single_stage\n",
    "\n",
    "# Plot results\n",
    "def plot_inference_times(avg_two_stage, avg_single_stage):\n",
    "    pipelines = ['Two-Stage Pipeline', 'Single-Stage Pipeline']\n",
    "    avg_times = [avg_two_stage, avg_single_stage]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(pipelines, avg_times, color=['blue', 'orange'])\n",
    "    plt.ylabel('Average Inference Time (seconds)', fontsize=12)\n",
    "    plt.xlabel('Pipeline', fontsize=12)\n",
    "    plt.title('Comparison of Inference Times', fontsize=14)\n",
    "    plt.ylim(0, max(avg_times) + 0.1)  # Add space above the highest bar\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add text on top of the bars\n",
    "    for i, time in enumerate(avg_times):\n",
    "        plt.text(i, time + 0.02, f\"{time:.2f} s\", ha='center', fontsize=10, color='black')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Measuring inference times...\")\n",
    "    avg_two_stage, avg_single_stage = measure_inference_times()\n",
    "    print(f\"Average inference time for Two-Stage Pipeline: {avg_two_stage:.2f} seconds\")\n",
    "    print(f\"Average inference time for Single-Stage Pipeline: {avg_single_stage:.2f} seconds\")\n",
    "\n",
    "    print(\"Plotting results...\")\n",
    "    plot_inference_times(avg_two_stage, avg_single_stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37d61ec2-2687-4127-8cab-50260339c778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Replace these with the actual calculated values\n",
    "avg_two_stage = 0.45  # Example value for the two-stage pipeline\n",
    "avg_single_stage = 0.30  # Example value for the single-stage pipeline\n",
    "\n",
    "# Function to plot the average inference times\n",
    "def plot_inference_times(avg_two_stage, avg_single_stage):\n",
    "    # Data for plotting\n",
    "    pipelines = ['Two-Stage Pipeline', 'Single-Stage Pipeline']\n",
    "    avg_times = [avg_two_stage, avg_single_stage]\n",
    "\n",
    "    # Plot settings\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(pipelines, avg_times, color=['blue', 'orange'])\n",
    "    plt.ylabel('Average Inference Time (seconds)', fontsize=12)\n",
    "    plt.xlabel('Pipeline', fontsize=12)\n",
    "    plt.title('Comparison of Inference Times', fontsize=14)\n",
    "    plt.ylim(0, max(avg_times) + 0.1)  # Add space above the highest bar\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Add text on top of the bars\n",
    "    for i, time in enumerate(avg_times):\n",
    "        plt.text(i, time + 0.02, f\"{time:.2f} s\", ha='center', fontsize=10, color='black')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "plot_inference_times(avg_two_stage, avg_single_stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017716a9-cc65-45f2-b6c0-6801ab26f9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 157 layers, 1763224 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Adding AutoShape... \n",
      "Using cache found in /Users/ashish/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-10-31 Python-3.11.5 torch-2.2.2 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5l summary: 267 layers, 46113663 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "yolo_nano = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/nano_weights/best.pt')\n",
    "yolo_large = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/large_weights/best.pt')\n",
    "\n",
    "def two_stage_detection_pipeline(frame):\n",
    "    # First-stage detection\n",
    "    results_nano = yolo_nano(frame)\n",
    "    detections_nano = results_nano.xyxy[0]\n",
    "\n",
    "    verified_detections = []\n",
    "\n",
    "    # Second-stage verification\n",
    "    for det in detections_nano:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "        if conf > 0.4:  # Confidence threshold for Nano\n",
    "            crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "            if crop.shape[0] > 0 and crop.shape[1] > 0:\n",
    "                results_large = yolo_large(crop)\n",
    "                detections_large = results_large.xyxy[0]\n",
    "\n",
    "                for det_large in detections_large:\n",
    "                    x1_l, y1_l, x2_l, y2_l, conf_l, cls_l = det_large.tolist()\n",
    "                    if conf_l > 0.5:  # Confidence threshold for Large\n",
    "                        verified_detections.append([x1, y1, x2, y2, conf_l, cls_l])\n",
    "\n",
    "    # Draw detections\n",
    "    for x1, y1, x2, y2, conf, cls in verified_detections:\n",
    "        label = yolo_large.names[int(cls)]\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    return frame\n",
    "\n",
    "# Live feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    processed_frame = two_stage_detection_pipeline(frame_rgb)\n",
    "\n",
    "    cv2.imshow('Live Detection', processed_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac8e17d-3861-4188-ae09-22413122afb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4212752451.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    verified_detections = []\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "yolo_nano = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/nano_weights/best.pt')  # First-stage model\n",
    "yolo_large = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/large_weights/best.pt')  # Second-stage model\n",
    "\n",
    "# Two-stage detection pipeline\n",
    "    verified_detections = []\n",
    "\n",
    "    # First-stage detection (YOLOv5 Nano)\n",
    "    start_time_nano = time.time()\n",
    "    results_nano = yolo_nano(frame)\n",
    "    detections_nano = results_nano.xyxy[0]  # [x1, y1, x2, y2, confidence, class]\n",
    "    end_time_nano = time.time()\n",
    "\n",
    "    # Second-stage verification (YOLOv5 Large)\n",
    "    start_time_large = time.time()\n",
    "    for det in detections_nano:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "\n",
    "        # Confidence threshold for Nano\n",
    "        if conf > 0.4:  # Adjust threshold as needed\n",
    "            # Crop region of interest (ROI) for second-stage model\n",
    "            crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "\n",
    "            # Skip resizing in YOLOv5 Large by passing original crop size\n",
    "            if crop.shape[0] > 0 and crop.shape[1] > 0:  # Ensure valid crop dimensions\n",
    "                results_large = yolo_large(crop, size=max(crop.shape[:2]))\n",
    "                detections_large = results_large.xyxy[0]\n",
    "\n",
    "                # Confirm detections\n",
    "                for det_large in detections_large:\n",
    "                    x1_l, y1_l, x2_l, y2_l, conf_l, cls_l = det_large.tolist()\n",
    "\n",
    "                    if conf_l > 0.5:  # Adjust threshold for Large\n",
    "                        # Map detection back to original coordinates\n",
    "                        verified_detections.append([x1, y1, x2, y2, conf_l, cls_l])\n",
    "    end_time_large = time.time()\n",
    "\n",
    "    # Visualize results on the frame\n",
    "    for x1, y1, x2, y2, conf, cls in verified_detections:\n",
    "        label = yolo_large.names[int(cls)]\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Print inference times\n",
    "    print(f\"Two-Stage Pipeline:\")\n",
    "    print(f\"  First-stage (Nano) inference time: {end_time_nano - start_time_nano:.2f} seconds\")\n",
    "    print(f\"  Second-stage (Large) inference time: {end_time_large - start_time_large:.2f} seconds\")\n",
    "\n",
    "    return frame\n",
    "\n",
    "# Live camera feed\n",
    "def live_camera_pipeline():\n",
    "    cap = cv2.VideoCapture(1)  # Open webcam (0 for default camera)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB for YOLO models\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Run the two-stage detection pipeline\n",
    "        processed_frame = two_stage_detection_pipeline(frame_rgb)\n",
    "\n",
    "        # Display the frame with detections\n",
    "        cv2.imshow('Two-Stage Detection (Live)', processed_frame)\n",
    "\n",
    "        # Press 'q' to exit the live feed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Start the live camera pipeline\n",
    "live_camera_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e53898-4808-44a1-a751-7a7c77cc4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "# Load models\n",
    "yolo_nano = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/nano_weights/best.pt')  # First-stage model\n",
    "yolo_large = torch.hub.load('ultralytics/yolov5', 'custom', path='/Users/ashish/Desktop/final_projects/suspicious activityy/code/large_weights/best.pt')  # Second-stage model\n",
    "\n",
    "# Two-stage detection pipeline\n",
    "def two_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # First-stage detection (YOLOv5 Nano)\n",
    "    start_time_nano = time.time()\n",
    "    results_nano = yolo_nano(image_rgb)\n",
    "    end_time_nano = time.time()\n",
    "    detections_nano = results_nano.xyxy[0]  # [x1, y1, x2, y2, confidence, class]\n",
    "\n",
    "    verified_detections = []\n",
    "\n",
    "    # Second-stage verification (YOLOv5 Large)\n",
    "    start_time_large = time.time()\n",
    "    for det in detections_nano:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "        \n",
    "        # Confidence threshold for Nano\n",
    "        if conf >= 0.25:  # Adjust threshold as needed\n",
    "            # Crop region of interest (ROI) for second-stage model\n",
    "            crop = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "            \n",
    "            # Run YOLOv5 Large on the cropped region\n",
    "            results_large = yolo_large(crop)\n",
    "            detections_large = results_large.xyxy[0]\n",
    "\n",
    "            # Confirm detections\n",
    "            for det_large in detections_large:\n",
    "                x1_l, y1_l, x2_l, y2_l, conf_l, cls_l = det_large.tolist()\n",
    "                \n",
    "                if conf_l > 0.5:  # Adjust threshold for Large\n",
    "                    # Map detection back to original coordinates\n",
    "                    verified_detections.append([x1, y1, x2, y2, conf_l, cls_l])\n",
    "    end_time_large = time.time()\n",
    "\n",
    "    # Visualize or save results\n",
    "    for x1, y1, x2, y2, conf, cls in verified_detections:\n",
    "        label = yolo_large.names[int(cls)]\n",
    "        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the final image\n",
    "    cv2.imshow('Detections (Two-Stage)', image)\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print inference times\n",
    "    print(f\"Two-Stage Pipeline:\")\n",
    "    print(f\"  First-stage (Nano) inference time: {end_time_nano - start_time_nano:.2f} seconds\")\n",
    "    print(f\"  Second-stage (Large) inference time: {end_time_large - start_time_large:.2f} seconds\")\n",
    "\n",
    "# Single-stage detection using YOLOv5 Large\n",
    "def single_stage_detection_pipeline(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Large model detection\n",
    "    start_time = time.time()\n",
    "    results_large = yolo_large(image_rgb)\n",
    "    end_time = time.time()\n",
    "\n",
    "    detections_large = results_large.xyxy[0]  # [x1, y1, x2, y2, confidence, class]\n",
    "\n",
    "    # Visualize or save results\n",
    "    for det in detections_large:\n",
    "        x1, y1, x2, y2, conf, cls = det.tolist()\n",
    "        if conf > 0.5:  # Confidence threshold for Large\n",
    "            label = yolo_large.names[int(cls)]\n",
    "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"{label} {conf:.2f}\", (int(x1), int(y1) - 10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the final image\n",
    "    cv2.imshow('Detections (Single-Stage)', image)\n",
    "    cv2.waitKey(5000)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Print inference time\n",
    "    print(f\"Single-Stage Pipeline:\")\n",
    "    print(f\"  YOLOv5 Large inference time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Test pipeline\n",
    "image_path = '/Users/ashish/Desktop/final_projects/suspicious activityy/dataset/OD-WeaponDetection/Weapons and similar handled objects/Sohas_weapon-Detection-YOLOv5/model_1_data/images/test/pistola_z3v5_09.jpg'\n",
    "two_stage_detection_pipeline(image_path)\n",
    "\n",
    "print(\"\\n--- Running Single-Stage Pipeline ---\")\n",
    "single_stage_detection_pipeline(image_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
